{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b446bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Step 2: Merge into a Single data.h5 File ===\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "with h5py.File('/train_catvnoncat.h5', 'r') as train_file, \\\n",
    "     h5py.File('/test_catvnoncat.h5', 'r') as test_file, \\\n",
    "     h5py.File('data.h5', 'w') as data_file:\n",
    "\n",
    "    data_file.create_dataset('train_set_x', data=np.array(train_file['train_set_x']))\n",
    "    data_file.create_dataset('train_set_y', data=np.array(train_file['train_set_y']))\n",
    "    data_file.create_dataset('test_set_x', data=np.array(test_file['test_set_x']))\n",
    "    data_file.create_dataset('test_set_y', data=np.array(test_file['test_set_y']))\n",
    "    data_file.create_dataset('list_classes', data=np.array(test_file['list_classes']))\n",
    "\n",
    "# === Step 3: Logistic Regression with Neural Network Mindset ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_dataset():\n",
    "    dataset = h5py.File(\"data.h5\", \"r\")\n",
    "    train_set_x_orig = np.array(dataset[\"train_set_x\"][:])\n",
    "    train_set_y_orig = np.array(dataset[\"train_set_y\"][:])\n",
    "    test_set_x_orig = np.array(dataset[\"test_set_x\"][:])\n",
    "    test_set_y_orig = np.array(dataset[\"test_set_y\"][:])\n",
    "    classes = np.array(dataset[\"list_classes\"][:])\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    return w, b\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -1/m * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
    "    dw = 1/m * np.dot(X, (A - Y).T)\n",
    "    db = 1/m * np.sum(A - Y)\n",
    "    cost = np.squeeze(cost)\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    return grads, cost\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(f\"Cost after iteration {i}: {cost}\")\n",
    "    params = {\"w\": w, \"b\": b}\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    return params, grads, costs\n",
    "\n",
    "def predict(w, b, X):\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    return (A > 0.5).astype(int)\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.005, print_cost=False):\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    print(f\"Train Accuracy: {100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100:.2f}%\")\n",
    "    return {\n",
    "        \"costs\": costs,\n",
    "        \"Y_prediction_test\": Y_prediction_test,\n",
    "        \"Y_prediction_train\": Y_prediction_train,\n",
    "        \"w\": w,\n",
    "        \"b\": b,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_iterations\": num_iterations\n",
    "    }\n",
    "\n",
    "# === Step 4: Prepare the Data and Train ===\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()\n",
    "\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "train_x = train_x_flatten / 255.\n",
    "test_x = test_x_flatten / 255.\n",
    "\n",
    "d = model(train_x, train_y, test_x, test_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n",
    "\n",
    "# === Step 5: Plot the Cost Curve ===\n",
    "plt.plot(d[\"costs\"])\n",
    "plt.ylabel(\"cost\")\n",
    "plt.xlabel(\"iterations (per hundreds)\")\n",
    "plt.title(f\"Learning rate = {d['learning_rate']}\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
