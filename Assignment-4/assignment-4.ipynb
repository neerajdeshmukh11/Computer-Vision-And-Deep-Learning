{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c41956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, layer_dims):\n",
    "        # Initialize parameters with He initialization\n",
    "        self.parameters = {}\n",
    "        self.L = len(layer_dims) - 1\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2. / layer_dims[l-1])\n",
    "            self.parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_derivative(self, Z):\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        Z = np.clip(Z, -500, 500)  # Prevent overflow\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = self.sigmoid(Z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        A = X\n",
    "        caches = []\n",
    "        for l in range(1, self.L):\n",
    "            Z = np.dot(self.parameters['W' + str(l)], A) + self.parameters['b' + str(l)]\n",
    "            A = self.relu(Z)\n",
    "            caches.append((A, Z))\n",
    "        ZL = np.dot(self.parameters['W' + str(self.L)], A) + self.parameters['b' + str(self.L)]\n",
    "        AL = self.sigmoid(ZL)\n",
    "        caches.append((AL, ZL))\n",
    "        return AL, caches\n",
    "\n",
    "    def compute_cost(self, AL, Y):\n",
    "        m = Y.shape[1]\n",
    "        epsilon = 1e-15\n",
    "        AL = np.clip(AL, epsilon, 1 - epsilon)  # Prevent log(0)\n",
    "        cost = -1 / m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "        return cost\n",
    "\n",
    "    def backward_propagation(self, X, Y, caches):\n",
    "        grads = {}\n",
    "        m = X.shape[1]\n",
    "        L = self.L\n",
    "        AL, ZL = caches[-1]\n",
    "        dAL = -(np.divide(Y, AL + 1e-15) - np.divide(1 - Y, 1 - AL + 1e-15))\n",
    "        dZL = dAL * self.sigmoid_derivative(ZL)\n",
    "        A_prev = caches[-2][0] if L > 1 else X\n",
    "        grads['dW' + str(L)] = 1/m * np.dot(dZL, A_prev.T)\n",
    "        grads['db' + str(L)] = 1/m * np.sum(dZL, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(self.parameters['W' + str(L)].T, dZL)\n",
    "\n",
    "        for l in reversed(range(1, L)):\n",
    "            A, Z = caches[l - 1]\n",
    "            A_prev = caches[l - 2][0] if l > 1 else X\n",
    "            dZ = dA_prev * self.relu_derivative(Z)\n",
    "            grads['dW' + str(l)] = 1/m * np.dot(dZ, A_prev.T)\n",
    "            grads['db' + str(l)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "            dA_prev = np.dot(self.parameters['W' + str(l)].T, dZ)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def train(self, X, Y, learning_rate=0.01, num_iterations=3000, print_cost=True):\n",
    "        costs = []\n",
    "        beta = 0.9\n",
    "        self.velocity = {}\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.velocity['dW' + str(l)] = np.zeros_like(self.parameters['W' + str(l)])\n",
    "            self.velocity['db' + str(l)] = np.zeros_like(self.parameters['b' + str(l)])\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            AL, caches = self.forward_propagation(X)\n",
    "            cost = self.compute_cost(AL, Y)\n",
    "            grads = self.backward_propagation(X, Y, caches)\n",
    "\n",
    "            for l in range(1, self.L + 1):\n",
    "                self.velocity['dW' + str(l)] = beta * self.velocity['dW' + str(l)] + (1 - beta) * grads['dW' + str(l)]\n",
    "                self.velocity['db' + str(l)] = beta * self.velocity['db' + str(l)] + (1 - beta) * grads['db' + str(l)]\n",
    "                self.parameters['W' + str(l)] -= learning_rate * self.velocity['dW' + str(l)]\n",
    "                self.parameters['b' + str(l)] -= learning_rate * self.velocity['db' + str(l)]\n",
    "\n",
    "            if print_cost and i % 100 == 0:\n",
    "                print(f\"Cost after iteration {i}: {cost}\")\n",
    "                costs.append(cost)\n",
    "\n",
    "        return costs\n",
    "\n",
    "    def predict(self, X):\n",
    "        AL, _ = self.forward_propagation(X)\n",
    "        predictions = (AL > 0.5).astype(int)\n",
    "        return predictions\n",
    "\n",
    "# Test the neural network with XOR problem\n",
    "np.random.seed(1)\n",
    "X = np.random.randn(2, 400)\n",
    "Y = (X[0, :] * X[1, :] > 0).astype(int).reshape(1, -1)\n",
    "\n",
    "# Create neural network with improved architecture\n",
    "layer_dims = [2, 10, 5, 1]  # 2 input features, 2 hidden layers (10 and 5 neurons), 1 output\n",
    "dnn = DeepNeuralNetwork(layer_dims)\n",
    "\n",
    "print(\"Training the neural network...\")\n",
    "costs = dnn.train(X, Y, learning_rate=0.1, num_iterations=3000, print_cost=True)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.plot(np.arange(0, 3000, 100), costs)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Iterations (hundreds)')\n",
    "plt.title('Learning Curve')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display accuracy\n",
    "predictions = dnn.predict(X)\n",
    "accuracy = np.mean(predictions == Y)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Visualize the decision boundary\n",
    "h = 0.01\n",
    "x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()].T\n",
    "Z = dnn.predict(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.3)\n",
    "plt.scatter(X[0, :], X[1, :], c=Y.flatten(), cmap=plt.cm.RdYlBu)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Decision Boundary of Neural Network (XOR Problem)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
